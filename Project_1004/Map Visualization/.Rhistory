#############################################################################
#b
# Create covariates and dataframe
txt <- reducedCorpus$documents$texts[1:804]
days <- date[1:804]
paper <- reducedCorpus$documents$paperName[1:804]
data <-data.frame(paper, txt, days)
##use STM's cleaning functions
processed <- textProcessor(data$txt, metadata=data, language="english", stem=TRUE)
##remove some words for speed purposes
out_50 <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=50)
fitSpec0<- stm(out_50$documents,out_50$vocab,K=0, init.type="Spectral",
content=~paper, prevalence = ~paper + as.numeric(days), max.em.its=30, data=out_50$meta, seed=2000)
out_60 <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=60)
###Run the model with the same number of topics as before
fitSpec0 <- stm(out_60$documents,out_60$vocab,K=0, init.type="Spectral",
content=~paper_name, prevalence = ~paper_name + as.numeric(days), max.em.its=30,
data=out_60$meta, seed=2000)
out_60 <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=50)
###Run the model with the same number of topics as before
fitSpec0 <- stm(out_60$documents,out_60$vocab,K=0, init.type="Spectral",
content=~paper_name, prevalence = ~paper_name + as.numeric(days), max.em.its=30,
data=out_60$meta, seed=2000)
fitSpec0<- stm(out_50$documents,out_50$vocab,K=0, init.type="Spectral",
content=~paper, prevalence = ~paper + as.numeric(days), max.em.its=30,
data=out_50$meta, seed=2000)
fitSpec0
plot.STM(fitSpec0, type="summary")
big_0<-c(1:5)
labelTopics(fitSpec0, big_0)
out_40 <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=40)#raise threshold
out_40$meta$papers<-as.factor(out_40$meta$paper)
out_40$meta$days<-as.numeric(out_40$meta$days)
##pick specifcation
prep<-estimateEffect(big_0 ~ papers , fitSpec0, meta=out_40$meta)
plot.estimateEffect(prep, covariate="papers", topics=big_0, model=out_40, method="difference",
cov.value1 = "guardian", cov.value2 = "mail",
xlab = "More mail......More guardian", xlim=c(-.1, .1))
##pick specifcation--over time
prep<-estimateEffect(big_0 ~ s(days) , fitSpec0, meta=out_40$meta)
##plot effects
plot.estimateEffect(prep, covariate="days", topics=big_0, model=out_40, method="continuous")
mydfm <- dfm(subset(inaugCorpus, Year>1900))
mydfm@Dimnames$docs
left_wing <- subset(inaugCorpus)[28]
right_wing <- subset(inaugCorpus)[18]
mydfm_new = textmodel_wordfish(mydfm, c(28,18))
mydfm_new
mydfm_new@theta
#theta is the distribution of topics over documents
mydfm_new@docs[mydfm_new@theta == max(mydfm_new@theta)] #place on right
mydfm_new@docs[mydfm_new@theta == min(mydfm_new@theta)] #place on left
mydfm_new@beta[mydfm_new@features == 'fascism']
freq_words<-mydfm_new@psi
freq_words
names(freq_words) <- mydfm_new@features
sort(freq_words)[1:50]
sort(freq_words, decreasing=T)[1:50]
##guitar plot
weights<-mydfm_new@beta # "wordscore"
plot(weights, freq_words)
for (i in 1:30){
j = align[i]# by Optimal assignment
matching_vec[i] = 0
for (m in 1:10){
for (n in 1:10){
if (top10_2[,j][m] == top10[,i][n]){
matching_vec[i] = matching_vec[i]+1
}
}
}
}
library('clue')
#Store the results of words over topics
##beta is the distribution of words over topics
words_topics1 = reducedCorpus_LDA@beta
words_topics2 = reducedCorpus_LDA_reset@beta
library(lda)
library(topicmodels)
library(stm)
library(LDAvis)
library(quanteda)
#library(quantedaData)
####################      0       Setup
require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
data(immigNewsCorpus, package = "quantedaData")
####################      1
######################################aaaaa######################################
summary(immigNewsCorpus,5)
topPapers = sort(table(immigNewsCorpus[[1]][['paperName']]), decreasing = TRUE)
reducedCorpus = subset(immigNewsCorpus, paperName %in% names(topPapers)[1:4])
load('/Users/XingCui/Desktop/DS_3001_Text_As_Data/Text_as_Data/HW3/custom_stopwords.RData')
custom_stopwords[573:588]
reducedCorpus_dfm = dfm(reducedCorpus, stem = T, removePunct=T, ignoredFeatures =custom_stopwords)
######################################ccccc######################################
reducedCorpus_dfm_trim = trim(reducedCorpus_dfm, minCount = 30, minDoc = 20)
######################################ddddd######################################
k = 30# # of topics
Seed = 2010#set seed as recitation
reducedCorpus_LDA = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed, burnin = 3,thin = 30, iter = 30)))
doc_topics = reducedCorpus_LDA@gamma
#arrange topics
max = apply(doc_topics, 1, which.max)
##write a function that finds the second max
which.max2 = function(x){
which(x == sort(x,partial=(k-1))[k-1])
}
max222<- apply(doc_topics, 1, which.max2)
max222<-sapply(max222, max)
##combine data
index = seq(1:1667)#index for data frame 392+412+511+352 = 1667
top2 = data.frame(max, max222,index)
guardian = data.frame(max[1:392], max222[1:392])#guardian index
daily_mail = data.frame(max[393:804], max222[393:804])#daily_mail index
#setting index for them
index_guardian = seq(1:392)
index_daily_mail = seq(1:412)
#plot
library(ggplot2)
z<-ggplot(guardian, aes(x=index_guardian, y=max.1.392., pch="First"))
z + geom_point(aes(x=index_guardian, y=max222.1.392., pch="Second") ) +theme_bw() + ylab("Topic Number")  + ggtitle("Guardian")  +
xlab(NULL) + theme(axis.ticks = element_blank(), axis.text.x = element_blank()) + geom_point() +
scale_shape_manual(values=c(18, 1), name = "Topic Rank for G")
z<-ggplot(daily_mail, aes(x=index_daily_mail, y=max.393.804., pch="First"))
z + geom_point(aes(x=index_daily_mail, y=max222.393.804., pch="Second") ) +theme_bw() + ylab("Topic Number")  + ggtitle("Daily Mail")  +
xlab(NULL) + theme(axis.ticks = element_blank(), axis.text.x = element_blank()) + geom_point() +
scale_shape_manual(values=c(18, 1), name = "Topic Rank for DM")
######################################ggggg######################################
guardian_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'guardian'),]
daily_mail_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'mail'),]
telegraph_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'telegraph'),]
times_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'times'),]
#use names of topics
election = c(mean(guardian_set[,3]),mean(daily_mail_set[,3]),mean(telegraph_set[,3]),mean(times_set[,3]))
race_war = c(mean(guardian_set[,25]),mean(daily_mail_set[,25]),mean(telegraph_set[,25]),mean(times_set[,25]))
politics = c(mean(guardian_set[,6]),mean(daily_mail_set[,6]),mean(telegraph_set[,6]),mean(times_set[,6]))
career_relocation = c(mean(guardian_set[,22]),mean(daily_mail_set[,22]),mean(telegraph_set[,22]),mean(times_set[,22]))
romance = c(mean(guardian_set[,2]),mean(daily_mail_set[,2]),mean(telegraph_set[,2]),mean(times_set[,2]))
install.packages('LDAvis')#Warning in install.packages : Perhaps you meant ‘LDAvis’ ?
install.packages("LDAvis")
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
set.seed(100)
# fit the standard LDA topic model
reducedCorpus_dfm_trim_fit <- LDA(reducedCorpus_dfm_trim, k = K, method = "Gibbs",
control = list(alpha = alpha, delta = eta))
LDApost <- posterior(reducedCorpus_dfm_trim_fit)
jsonLDA <- createJSON(phi = LDApost$terms,
theta = LDApost$topics,
doc.length = ntoken(reducedCorpus_dfm_trim),
vocab = features(reducedCorpus_dfm_trim),
term.frequency = colSums(reducedCorpus_dfm_trim))
serVis(jsonLDA, out.dir = "fancyVisualization", open.browser = TRUE)
Seed2 = 3000
reducedCorpus_LDA_reset = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed2, burnin = 3,thin = 30, iter = 30)))
######################################bbbbb######################################
install.packages('clue')
library('clue')
install.packages("clue")
words_topics1 = reducedCorpus_LDA@beta
words_topics2 = reducedCorpus_LDA_reset@beta
align = clue::solve_LSAP(words_topics1%*%t(words_topics2), maximum=TRUE)
align
#sort(align)
######################################ccccc######################################
top10
top10_2 <- get_terms(reducedCorpus_LDA_reset, k=10)
top10_2
top10 = get_terms(reducedCorpus_LDA,k = 10)#k is The maximum number of terms/topics returned
top10#storing top10
matching_vec = vector()
for (i in 1:30){
j = align[i]# by Optimal assignment
matching_vec[i] = 0
for (m in 1:10){
for (n in 1:10){
if (top10_2[,j][m] == top10[,i][n]){
matching_vec[i] = matching_vec[i]+1
}
}
}
}
matching_vec #0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5
mean(matching_vec)#0.1666667, sad
k = 10
#Reset seeds
Seed3 = 3001#our course
Seed4 = 1003#another course I am suffering
#actually just wanna see difference in big gap of seeds
reducedCorpus_LDA_seed3 = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed3, burnin = 3,thin = 30, iter = 30)))
reducedCorpus_LDA_seed4 = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed4, burnin = 3,thin = 30, iter = 30)))
words_topics3 = reducedCorpus_LDA_seed3@beta
words_topics4 = reducedCorpus_LDA_seed4@beta
align2 = clue::solve_LSAP(words_topics3%*%t(words_topics4), maximum=TRUE)
align2 #1 => 10, 2 => 5, 3 => 6, 4 => 9, 5 => 8, 6 => 1, 7 => 2, 8 => 4, 9 => 3, 10 => 7
top10_3 = get_terms(reducedCorpus_LDA_seed3, k=10)
top10_4 = get_terms(reducedCorpus_LDA_seed4, k=10)
matching_vec2 = vector()
for (i in 1:10){
j = align2[i]# by Optimal assignment
matching_vec2[i] = 0
for (m in 1:10){
for (n in 1:10){
if (top10_4[,j][m] == top10_3[,i][n]){
matching_vec2[i] = matching_vec2[i]+1
}
}
}
}
matching_vec2 #0 0 0 0 0 0 0 0 0 5
mean(matching_vec2) # 0.5
date <- reducedCorpus$documents$day
date
######################################bbbbb######################################
texts <- reducedCorpus$documents$texts[1:804] #392+412 = 804
days <- date[1:804]
paper_name <- reducedCorpus$documents$paperName[1:804]
data <- data.frame(paper_name, texts, days) #df
##use STM's cleaning functions
processed <- textProcessor(data$texts, metadata=data, language="english", stem=TRUE)#NEVER USE "E"nglish
##remove some words for speed purposes
out_40 <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=40)#raise threshold
###Run the model with the same number of topics as before
fitSpec0 <- stm(out_40$documents,out_40$vocab,K=0, init.type="Spectral",
content=~paper_name, prevalence = ~paper_name + as.numeric(days), max.em.its=30,
data=out_40$meta, seed=2000)
fitSpec0 #A topic model with 50 topics, 804 documents and a 1113 word dictionary.
plot.STM(fitSpec0, type="summary")
big_0<-c(1:5)
labelTopics(fitSpec0, big_0)
out_40$meta$papers<-as.factor(out_40$meta$paper)
out_40$meta$days<-as.numeric(out_40$meta$days)
##pick specifcation
prep<-estimateEffect(big_0 ~ papers , fitSpec0, meta=out_40$meta)
##plot effects
plot.estimateEffect(prep, covariate="papers", topics=big_0, model=out_40, method="difference",
cov.value1 = "guardian", cov.value2 = "mail",
xlab = "More mail......More guardian", xlim=c(-.1, .1))
prep<-estimateEffect(big_0 ~ s(days) , fitSpec0, meta=out_40$meta)
##plot effects
plot.estimateEffect(prep, covariate="days", topics=big_0, model=out_40, method="continuous")
plot.STM(fitSpec25, type="perspectives", topics = 12)
plot.STM(fitSpec0, type="perspectives", topics = 12)
mydfm = dfm(subset(inaugCorpus, Year>1900))
######################################bbbbb######################################
#check document location
mydfm@Dimnames$docs #obama[28],nixon[18]
left_wing = subset(inaugCorpus)[28]
right_wing = subset(inaugCorpus)[18]
######################################ccccc######################################
#http://www.wordfish.org/uploads/1/2/9/8/12985397/wordfish_manual.pdf
#page 8
mydfm_new = textmodel_wordfish(mydfm, c(28,18))
mydfm_new
mydfm_new@theta
#theta is the distribution of topics over documents
mydfm_new@docs[mydfm_new@theta == max(mydfm_new@theta)] #"1909-Taft", place on right
mydfm_new@docs[mydfm_new@theta == min(mydfm_new@theta)] #"1993-Clinton", place on left
######################################ddddd######################################
mydfm_new@beta[mydfm_new@features == 'fascism'] #-4.291483
######################################eeeee######################################
##most important features--word fixed effects
freq_words<-mydfm_new@psi
freq_words
names(freq_words) = mydfm_new@features
sort(freq_words)[1:50]
sort(freq_words, decreasing=T)[1:50]
##guitar plot
weights = mydfm_new@beta # "wordscore"
plot(weights, freq_words)
z<-ggplot(guardian, aes(x=index_guardian, y=max.1.392., pch="First"))
library(lda)
library(topicmodels)
library(stm)
library(LDAvis)
library(quanteda)
#library(quantedaData)
####################      0       Setup
require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
data(immigNewsCorpus, package = "quantedaData")
####################      1
######################################aaaaa######################################
summary(immigNewsCorpus,5)
topPapers = sort(table(immigNewsCorpus[[1]][['paperName']]), decreasing = TRUE)
reducedCorpus = subset(immigNewsCorpus, paperName %in% names(topPapers)[1:4])
table(reducedCorpus[[1]][["paperName"]])
######################################bbbbb######################################
load('/Users/XingCui/Desktop/DS_3001_Text_As_Data/Text_as_Data/HW3/custom_stopwords.RData')
custom_stopwords[573:588]
reducedCorpus_dfm = dfm(reducedCorpus, stem = T, removePunct=T, ignoredFeatures =custom_stopwords)
######################################ccccc######################################
reducedCorpus_dfm_trim = trim(reducedCorpus_dfm, minCount = 30, minDoc = 20)
######################################ddddd######################################
k = 30# # of topics
Seed = 2010#set seed as recitation
reducedCorpus_LDA = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed, burnin = 3,thin = 30, iter = 30)))
######################################eeeee######################################
top10 = get_terms(reducedCorpus_LDA,k = 10)#k is The maximum number of terms/topics returned
top10#storing top10
top5 = get_topics(reducedCorpus_LDA)
#sort(table(top5),decreasing = FALSE) #Top5:{13:17,  16:29,  27:30,  23:31,   9:32}
sort(table(top5),decreasing = TRUE) #{3:134  25:129   6:89  22:88   2:85 }
#name them
top10[,3] #election:"parti"   "voter"   "ukip"    "labour"  "elect"   "vote"    "conserv" "tori"    "polit"   "support"
top10[,25] #race war: "ukip"     "farag"    "parti"    "nigel"    "racist"   "european" "leader"   "elect"    "campaign" "candid"
top10[,6] #politics:"court"  "polic"  "judg"   "law"    "year"   "deport" "crimin" "crime"  "human"  "justic"
top10[,22] #career relocation:"offic"    "home"     "travel"   "case"     "border"   "investig" "passeng"  "airport"  "staff"    "passport"
top10[,2] #romance:"love"  "book"  "music" "feel"  "john"  "time"  "call"  "great" "hes"   "star"
guardian_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'guardian'),]
daily_mail_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'mail'),]
telegraph_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'telegraph'),]
times_set = doc_topics[which(reducedCorpus[[1]]['paperName'] == 'times'),]
#use names of topics
election = c(mean(guardian_set[,3]),mean(daily_mail_set[,3]),mean(telegraph_set[,3]),mean(times_set[,3]))
race_war = c(mean(guardian_set[,25]),mean(daily_mail_set[,25]),mean(telegraph_set[,25]),mean(times_set[,25]))
politics = c(mean(guardian_set[,6]),mean(daily_mail_set[,6]),mean(telegraph_set[,6]),mean(times_set[,6]))
career_relocation = c(mean(guardian_set[,22]),mean(daily_mail_set[,22]),mean(telegraph_set[,22]),mean(times_set[,22]))
romance = c(mean(guardian_set[,2]),mean(daily_mail_set[,2]),mean(telegraph_set[,2]),mean(times_set[,2]))
election
race_war
politics
career_relocation
romance
election = c(mean(guardian_set[,13]),mean(daily_mail_set[,13]),mean(telegraph_set[,13]),mean(times_set[,13]))
election
table(reducedCorpus[[1]][["paperName"]])
plot.STM(fitSpec0, type="summary")
plot.estimateEffect(prep, covariate="days", topics=big_0, model=out_40, method="continuous")
###Let's see how the terms used vary within a topic
plot.STM(fitSpec0, type="perspectives", topics = 12)
big_0[1]
library(lda)
library(topicmodels)
library(stm)
library(LDAvis)
library(quanteda)
#library(quantedaData)
####################      0       Setup
require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
data(immigNewsCorpus, package = "quantedaData")
####################      1
######################################aaaaa######################################
summary(immigNewsCorpus,5)
topPapers = sort(table(immigNewsCorpus[[1]][['paperName']]), decreasing = TRUE)
reducedCorpus = subset(immigNewsCorpus, paperName %in% names(topPapers)[1:4])
table(reducedCorpus[[1]][["paperName"]])
mydfm = dfm(subset(inaugCorpus, Year>1900))
######################################bbbbb######################################
#check document location
mydfm@Dimnames$docs #obama[28],nixon[18]
left_wing = subset(inaugCorpus)[28]
right_wing = subset(inaugCorpus)[18]
######################################ccccc######################################
left_wing
mydfm_new = textmodel_wordfish(mydfm, c(28,18))
mydfm_new
mydfm_new@theta
#theta is the distribution of topics over documents
mydfm_new@docs[mydfm_new@theta == max(mydfm_new@theta)] #"1909-Taft", place on right
mydfm_new@docs[mydfm_new@theta == min(mydfm_new@theta)] #"1993-Clinton", place on left
min(mydfm_new@theta)
max(mydfm_new@theta)
load('/Users/XingCui/Desktop/DS_3001_Text_As_Data/Text_as_Data/HW3/custom_stopwords.RData')
custom_stopwords[573:588]
reducedCorpus_dfm = dfm(reducedCorpus, stem = T, removePunct=T, ignoredFeatures =custom_stopwords)
######################################ccccc######################################
reducedCorpus_dfm_trim = trim(reducedCorpus_dfm, minCount = 30, minDoc = 20)
######################################ddddd######################################
k = 30# # of topics
Seed = 2010#set seed as recitation
reducedCorpus_LDA = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed, burnin = 3,thin = 30, iter = 30)))
######################################eeeee######################################
top10 = get_terms(reducedCorpus_LDA,k = 10)#k is The maximum number of terms/topics returned
top10#storing top10
top5 = get_topics(reducedCorpus_LDA)
#sort(table(top5),decreasing = FALSE) #Top5:{13:17,  16:29,  27:30,  23:31,   9:32}
sort(table(top5),decreasing = TRUE) #{3:134  25:129   6:89  22:88   2:85 }
Seed2 = 3000
reducedCorpus_LDA_reset = (Gibbs = LDA(reducedCorpus_dfm_trim, method = 'Gibbs', k = k,
control = list(seed = Seed2, burnin = 3,thin = 30, iter = 30)))
library('clue')
words_topics1 = reducedCorpus_LDA@beta
words_topics2 = reducedCorpus_LDA_reset@beta
words_topics1
words_topics2
align = clue::solve_LSAP(words_topics1%*%t(words_topics2), maximum=TRUE)
align
library(plyr)         # To manipulate data
library(ggplot2)      # To have ggplot2 graphic interface
library(lattice)      # To have Lattice graphic interface
library(sp)
library(rgdal)        # To load "shapefiles" into R and use in conversions of spatial formats
library(rgeos)        # To use in geometrical operations
library(spatstat)     # To use with Voronoi Tesselation
library(sp)           # Methods for retrieving coordinates from spatial objects.
library(maptools)     # A package for building maps
library(maps)         # A package for building maps
library(RColorBrewer) # To build RColorBrewer color palettes
library(grDevices)    # To build grDevices color palettes
library(reshape2)     # To have more flexibility when transforming data
library(rCharts)      # To create and customize interactive javascript visualizations in R
library(knitr)        # For dynamic report generation in R
library(base64enc)    # Tools for base64 encoding
suppressPackageStartupMessages(library(googleVis)) # To use with Google visualization in R
setwd('/Users/XingCui/Desktop/DS_1004_Big_Data/Project/Neighborhood Tabulation Areas/')
norway2 <- readOGR(dsn = "." ,"geo_export_b9ed049f-9a51-476c-bbfa-739885e2831f")
norway2_data <- norway2@data
str(norway2_data); head(norway2_data)
u15_new  = read.csv('u15_new.csv',col.names = c('x','ids','count'))
d <- norway2_data$ntaname
e = u15_new$count
#e = log(e+1)
#e <- rnorm(length(norway2_data$ntaname),1000)
name3 <- c("NAME_1", "Churn")
dt2 <- as.data.frame(cbind(d, e), stringsAsFactors=TRUE)
dt2$e <- as.numeric(dt2$e); colnames(dt2) <- name3; churn <- dt2
IDs <- u14_new$ids#used to be dt2$d
norway3_new <- unionSpatialPolygons(norway2, IDs)
IDs <- u15_new$ids#used to be dt2$d
norway3_new <- unionSpatialPolygons(norway2, IDs)
norway4_new <- SpatialPolygonsDataFrame(norway3_new, churn)
pal2 <- colorRampPalette(c("linen", "Navy"))
trellis.par.set(axis.line=list(col=NA))# Remove the plot frame
spplot(norway4_new, "Churn", main="Uber 2015 Jan-Jun in NYC", # Plot the regions with Lattice
lwd=.4, col="white", col.regions=pal2(19), as.table = TRUE,#border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
u14_new  = read.csv('ub14haha.csv',col.names = c('x','ids','count','cc'))
u14_new  = read.csv('ub14haha.csv',col.names = c('x','ids','count','cc'))
d <- norway2_data$ntaname
e14 = u14_new$cc
name3 <- c("NAME_1", "Churn")
dt2 <- as.data.frame(cbind(d, e14), stringsAsFactors=TRUE)
dt2$e <- as.numeric(dt2$e14); colnames(dt2) <- name3; churn <- dt2
IDs14 <- u15_new$ids#used to be dt2$d
u14_new  = read.csv('ub14haha.csv',col.names = c('x','ids','count','cc'))
u14_new  = read.csv('ub14haha.csv',col.names = c('x','y','ids','count','cc'))
d <- norway2_data$ntaname
View(u14_new)
e14 = u14_new$cc
#e = log(e+1)
#e <- rnorm(length(norway2_data$ntaname),1000)
name3 <- c("NAME_1", "Churn")
dt24 <- as.data.frame(cbind(d, e14), stringsAsFactors=TRUE)
dt2$e14 <- as.numeric(dt2$e14); colnames(dt2) <- name3; churn4 <- dt24
e14
dt24 <- as.data.frame(cbind(d, e14), stringsAsFactors=TRUE)
dt2$e14 <- as.numeric(dt24$e14); colnames(dt24) <- name3; churn4 <- dt24
IDs14 <- u14_new$ids#used to be dt2$d
norway3_new <- unionSpatialPolygons(norway2, IDs14)
norway4_new <- SpatialPolygonsDataFrame(norway3_new, churn4)
pal2 <- colorRampPalette(c("linen", "darkorchid4"))
trellis.par.set(axis.line=list(col=NA))# Remove the plot frame
spplot(norway4_new, "Churn", main="Uber 2014 Apr-Sept in NYC", # Plot the regions with Lattice
lwd=.4, col="white", col.regions=pal2(19), as.table = TRUE,#border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
taxi_dist  = read.csv('taxi_dist_heat_new.csv',col.names = c('ids','count'))
#q = taxi_dist$ids
d <- norway2_data$ntaname
eee = taxi_dist$count
name33 <- c("NAME_1", "Churn")
dt22 <- as.data.frame(cbind(d, eee), stringsAsFactors=TRUE)
dt22$eee <- as.numeric(dt22$eee)
colnames(dt22) <- name33
churn <- dt22
arrow = list("SpatialPolygonsRescale", layout.north.arrow(),
offset = c(-74.2,40.85), scale = 0.5, which = 2)
IDsss <- taxi_dist$ids#used to be dt2$d
norway33_new <- unionSpatialPolygons(norway2, IDsss)
norway44_new <- SpatialPolygonsDataFrame(norway33_new, churn)
pal2 <- colorRampPalette(c("aliceblue", "goldenrod4"))
trellis.par.set(axis.line=list(col=NA))# Remove the plot frame
spplot(norway44_new, "Churn", main="Taxi Average in NYC", # Plot the regions with Lattice
lwd=.4, col="white", col.regions=pal2(19), as.table = TRUE,sp.layout = list(arrow),#border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
prediction  = read.csv('newfinal.csv',col.names = c('x','label','ids','name'))
d <- norway2_data$ntaname
e = prediction$label
name3 <- c("NAME_1", "Churn")
dt2 <- as.data.frame(cbind(d, e), stringsAsFactors=TRUE)
dt2$e <- as.numeric(dt2$e); colnames(dt2) <- name3; churn <- dt2
IDs <- prediction$ids#used to be dt2$d
norway3_new <- unionSpatialPolygons(norway2, IDs)
norway4_new <- SpatialPolygonsDataFrame(norway3_new, churn)
#color = c('steelblue1','slateblue','orangered3','khaki')
pal2 <- colorRampPalette(c("aliceblue", "darkcyan"))#linen
trellis.par.set(axis.line=list(col=NA))# Remove the plot frame
spplot(norway4_new, "Churn", main="Uber VS Taxi", # Plot the regions with Lattice
lwd=.4, col="black", as.table = TRUE, col.regions=pal2(19),#, #border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
spplot(norway4_new, "Churn", main="Uber VS Taxi", # Plot the regions with Lattice
lwd=.4, col="black", as.table = TRUE, col.regions=pal2(19),#, #border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
spplot(norway4_new, "Churn", main="Uber Now and in the Future", # Plot the regions with Lattice
lwd=.4, col="black", as.table = TRUE, col.regions=pal2(19),#, #border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
spplot(norway4_new, "Churn", main="Uber VS Taxi", # Plot the regions with Lattice
lwd=.4, col="black", as.table = TRUE, col.regions=color,#pal2(19),#, #border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
color = c('steelblue1','slateblue','orangered3','khaki')
#pal2 <- colorRampPalette(c("aliceblue", "darkcyan"))#linen
trellis.par.set(axis.line=list(col=NA))# Remove the plot frame
spplot(norway4_new, "Churn", main="Uber VS Taxi", # Plot the regions with Lattice
lwd=.4, col="black", as.table = TRUE, col.regions=color,#pal2(19),#, #border(outside = TRUE),
colorkey = TRUE, scales = list(draw = TRUE), bty="n")
